\chapter{Literature Review\label{cha:chapter2}}
This chapter presents a comprehensive review of the important literature relevant to the key topics of this thesis. Initially, it examines the evolution of access control. This is followed by an in-depth exploration of distributed event stores, elucidating their development, their strengths and weaknesses, and their prominence in modern distributed database systems. Subsequently, the following section explores data anonymization, highlighting general techniques as well as those specifically tailored for streaming data.


\section{Access Control\label{sec:rbac}}
Access control is a fundamental concept in information security, defining the actions that a subject, typically a user or an automated process run by a user, is authorized to perform on an object, which could be data, files, or other system resources. These actions include a variety of operations, traditionally including but not limited to, reading, writing, and executing files or data. As a critical component of system security, the methodologies and principles have been subject to extensive research and evolution over several decades. Ever adapting to the change of requirements and emerging security concerns.\par
Sahndu and Samarati's seminal work \cite{Sandhu1994} lays the foundational framework for understanding access control in the broader context of information security. They emphasize access control as an integral component of overall security, closely linked with authentification and auditing. They go on to describe the basic principles of access control, distinguishing between policies and mechanisms, focusing on the former. First, they explain the Access Control Matrix and their derivatives \acp{ACL} and Capabilities lists and ultimately Authorization Relations. The Access Control Matrix is a basic security model where rows represent subjects and columns represent objects. Each cell in this matrix specifies the actions a subject can perform on an object. However, this matrix often contains many empty cells as not all subjects interact with all objects. \acp{ACL} offer a streamlined approach by listing objects and specifying which users have what permissions. Conversely, Capabilities Lists provide a subject-focused perspective, listing subjects and their permissions for different objects. Furthermore, they analyze the prevailing standards of their time: \ac{DAC} and \ac{MAC}. In \ac{DAC} permissions are assigned by the resource owner typically in the form of \acp{ACL}. A resource owner is also at liberty to delegate the task of granting permissions to another subject. Thus, this approach does not facilitate control over the dissemination of information, however, the decentralized approach provides flexibility. \ac{MAC} on the other hand relies on a central authority to decide on all matters related to permissions. With roots in the US military, it drew inspiration from the need-to-know principle. In \ac{MAC} the flow of information is strictly regulated. Overall, the authors criticize both approaches to access control regarding their adaptability and scope. The mandatory approach was considered too rigid, the discretionary model was largely confined to research applications due to its cooperative yet autonomous focus. They appreciate the newcomer \ac{RBAC} in the space, for its discretionary flexibility and its mandatory strictness, providing a wide range of applicability, especially in commercial enterprises. A key benefit of \ac{RBAC} lies in its facilitation of smooth transitions of permissions and privileges when a user's role within an organization changes. Moreover, \ac{RBAC} simplifies the implementation of separation of duties, through mutually exclusive roles. Sahndu elaborates on \ac{RBAC} further together with Coyne \cite{Sandhu_Coyne_1994} explaining that finding a consensus in the form of a \ac{RBAC} standard requires a multidimensional view, stating that considerations regarding the nature of privileges and permissions, hierarchical roles, user assignments, privilege and permission assignment, role usage, role evolution as well as object attributes have to be made. \par
A comprehensive understanding of \ac{RBAC} requires the distinction between user groups and roles. Permissions based on user groups are a long-established practice - but do not offer the same range of functionality as role-based permissions. Ferraiolo and Kuhn \cite{ferraiolo1992role} identify two distinct differences, the first being that groups function as a discretionary mechanism, unlike roles. Access rights are assigned at the liberty of the object owner, with groups comprising users to whom the owner grants access. In contrast, roles represent a more abstract categorization of the user, allowing a single user to be associated with multiple roles. Unlike group-based access, permissions in a role-based model are assigned based on the roles themselves, not at the discretion of the resource owner. The second key difference lies in the operations associated with the permission. Groups are assigned classical file permissions as is common for an operating system e.g. read, write, execute, and own. Roles, on the other hand, refine this approach by defining 'transactions' - the authorization to execute a specific function on a set of data items. This allows for a much more nuanced and finetuned approach to access control, aligning more closely with practical requirements and daily operations. \par
Building on these insights, Sandhu further solidifies the concept of \ac{RBAC} in his pivotal work 'Role-based access control' \cite{sandhu1998role} laying the cornerstone for the formulation of a standard. In 2001, Ferraiolo et al. \cite{Ferraiolo2001} proposed a NIST standard for role-based access control, with Sandhu notably listed as a contributing author. Their goal was to bring clarity and establish well-founded, common terminology in the field of role-based access control. Their standard introduces four levels of \ac{RBAC}, each building upon the other. The foundational level, termed, 'Core \ac{RBAC}', includes basic data elements, namely users, roles, objects, operations, and permissions. Users are assigned to roles. Roles in turn are assigned permissions. Permissions are permissible operations to objects. In any given session, a user operates under a specific subset of their assigned roles. This core concept is then expanded to include hierarchies of roles, static separations of duties, and, ultimately, dynamic separations of duties. \par
Access control methodologies continue to evolve, with fine-grained access control emerging as a particularly popular approach due to its enhanced granularity in permission settings. Wang et al. \cite{wang2007correctness} examined the correctness of fine-grained access control, formulating the requirements of them being sound and secure to achieve maximum information. Other concepts include purpose-based access control for privacy protection \cite{Byun2008}. \par
An especially interesting inquiry relevant to this thesis is raised by Chaudhuri et al. in \cite{chaudhuri2011database}. They ask whether there is common ground between database access control and privacy. Despite their apparent relation, these two are seldom addressed together. In their journal article, they expand on the differential privacy notion with noisy views. Differential privacy requires that computations are formally equivalent when performed with or without any single record. Noisy views refer to the technique of adding noise to aggregate data in a database, thereby enhancing privacy. This innovative combination implemented on a database server level seems promising in bridging the gap between access control and privacy. This approach of harmonizing access control and privacy aligns closely with the objectives pursued in this thesis.



\section{Distributed Event Stores}
In modern times data is being produced at an unprecedented rate and volume. A significant portion of that data is so-called 'streaming data'. Streaming data is continuously generated, often in high volumes and at high velocity, from various sources. Its distinguishing characteristic, however, lies in its continuous flow and boundless nature. It additionally requires real-time or near-real-time processing as relevancy is key. Sources of streaming data include \ac{IoT} devices, log files, financial transactions, and social media platforms. One of the key challenges of streaming data is the need for systems that can process and analyze the data as soon as it arrives. This difficulty is exacerbated by the volume, velocity, and variety (the three Vs of Big Data) of the data. Moreover, streaming data often requires a different approach to data management and storage. Since the data is continually flowing, it is infeasible to store all incoming data indefinitely. \par
Traditionally, relational databases have been the go-to with Oracle \cite{oracle} and MySQL \cite{mysql} as the dominating database management systems \cite{dbms_statistics}. Over time, relational databases have evolved. Initially, their primary focus was enabling high-speed transaction processing, as illustrated by Selinger et al. in 1979 \cite{selinger1979access}. The emergence of unstructured data, as is common in streaming data, for instance, has led to a shift away from relational databases to column-based or more generally NoSQL databases. Prominent examples of such databases include Apache Cassandra \cite{cassandra}, Cloud Bigtable \cite{cloud_bigtable}, Amazon Redshift \cite{amazon_redshift}, and MongoDB \cite{mongo_db}. The immediate processing and decision-making requirements of streaming data can oftentimes not be met by these types of databases. Specifically, managing time-ordered events and executing temporal queries present significant challenges. Additionally, streaming data necessitates robust transactional guarantees in addition to complex event processing capabilities. This poses another challenge to NoSQL databases. Ultimately, the high throughput and low latency demands on top of the aforementioned requirements are enough to require a new model entirely. \par
The architectural pattern of event sourcing emerges. It involves storing the state changes of an application as a sequence of events. Instead of keeping the current state of data in a database, every change (or event) that affects the system's state is captured and stored. These events are immutable, meaning once they are stored they cannot be changed. A principal advantage of event sourcing thus lies in the reproducibility of system changes. Events can be replayed to reconstruct the system state at any point in time. This is particularly useful for debugging, auditing, and understanding the sequence of actions that led to a particular state. \par
Numerous entities have integrated event sourcing into their database designs. Notably, Greg Young, a pioneer of event sourcing, developed EventStoreDB \cite{eventstore_db}. Nowadays, there are commercial as well as open-source solutions available, offering comprehensive capabilities for event storage. To facilitate scalability and fault-tolerance most opt for a distributed approach. Prominent \acp{DES} include Apache Kafka \cite{kafka}, Amazon Kinesis \cite{amazon_kinesis}, RabbitMQ \cite{rabbit_mq} and Apache Pulsar \cite{pulsar}.

\section{Anonymization\label{sec:anon}}
Protecting privacy is an increasing concern for companies that rely on processing data. With extensive legislation in place across the globe, data scientists and data engineers alike are tasked with securing their users' data. The key requirement for privacy is restricting the reidentification of an individual with a record. Typically, the attributes in a single record can be categorized into direct identifiers also called \ac{PII}, indirect identifiers called quasi-identifiers, and the remaining, sensitive data. Anonymization takes on different forms in achieving the task of preventing reidentification. One approach is to use cryptographic methods to make it unreadable for individuals without access to the key. Depending on the concrete cryptographic function, this can be an expensive yet secure way of doing it. While Cryptography is a fascinating field at the intersection of computer science and mathematics, its application for streaming data appears limited due to its costly nature, with the cost in performance making it unattractive for distributed event stores. An alternative, yet effective approach involves employing masking functions, which alter the original data values, thereby obfuscating them and making it more challenging for unauthorized entities to discern the true information. In contrast to cryptographic functions, this is generally a cheaper approach, but the resulting protection is highly dependent on the masking function and its application. In the following, we survey the most prevalent masking functions, describing their operational principles and applications. Subsequently, we address particular considerations that are to be made in the context of data streaming, underscoring how these principles adapt to the dynamic nature of such environments.

\subsection{Masking Functions\label{sec:masking_functions}}
Initially, we establish a basic understanding of key terms essential for this discussion. In this context, a \textit{datum} refers to a single piece of information e.g. a singular entry of a database or any one event of a data stream. Synonymous with it also used the word \textit{tuple}. A tuple is composed of one or multiple \textit{attributes}. The names of the attributes are typically used as keys for identification of the attribute within the tuple. It is customary for individual tuples to be part of a bigger collection. In the static context, they are collected in databases and grouped in tables. Data streams work analogous to dynamic operations. All tuples of the same database table or data stream are required to follow the same pattern. This refers to the sequence of attributes of each tuple. This pattern is fixed in a data schema associated with the table or stream. Sometimes it is appended to each datum in the form of a header. As this substantially increases the size of each datum it is more common to define it once in the initialization step of the database table or data stream. \par
Having established the terminology let us begin by introducing prevalent masking functions:\par

\textbf{Suppression} aims to effectively delete the value of a tuple's attribute by replacing it with a meaningless character, most commonly the asterisk \textit{*}. It is important to note, that actually removing the attribute from the tuple or replacing it with a null value would violate the data schema and thus negatively impact operability. The asterisk does the trick while maintaining the data schema. To work, Suppression only requires a non-empty set of keys for the attributes that are supposed to be suppressed as parameters. Naturally, it leads to total information loss of the specified fields. This can be particularly useful for fields containing \ac{PII} like a person's home address as shown in Figure \ref{fig:suppression}. 

\bigskip

\begin{figure}[ht]
    \begin{center}
    \footnotesize{
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|}
            \hline
            address \\
            \hline
            Smith Street 3 \\
            \hline
            \end{tabular}
            \quad $\longrightarrow$ \quad
            \begin{tabular}{|c|}
            \hline
            address \\
            \hline
            * \\
            \hline
        \end{tabular}
    }
    \end{center}
    \caption{Example of suppression of an attribute.\label{fig:suppression}}
\end{figure}

A similar approach is applied in \textbf{Blurring}. Here, the value of an attribute is replaced with arbitrary characters, typically \textit{X}s. It is distinguishable from Suppression in that not all characters of the value have to be replaced and even the amount of characters can remain the same. Imagine a user at the checkout of an online store that they have already purchased goods at prior to this session. Here the credit card information of that user was saved as part of the agreement from the previous session. The user is then given the option to use that credit card again, with it being specified as a sequence of blurred characters with only the last three digits in plain text as shown in Figure \ref{fig:blurring}. This allows the user to double-check the card information without exposing the credit card number to the network, screen captures, or bystanders. This operation also leads to high information loss but retains some usability of the original value. The parameters for Blurring include the keys to blur as well as optionally the number of characters and whether the amount of characters is to be maintained. Note, that setting the parameters to blur all characters and reduce the amount to one is equal in functionality to Suppression. 

\bigskip

\begin{figure}[ht]
    \begin{center}
    \footnotesize{
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|}
            \hline
            credit card \\
            \hline
            1234 5678 9123 4567 \\
            \hline
            \end{tabular}
            \quad $\longrightarrow$ \quad
            \begin{tabular}{|c|}
            \hline
            credit card \\
            \hline
            XXXX XXXX XXXX X567 \\
            \hline
        \end{tabular}
    }
    \end{center}
    \caption{Example of blurring of an attribute.\label{fig:blurring}}
\end{figure}

\textbf{Substitution} replaces the value of the specified attribute with a predefined substitute. Figure \ref{fig:substitution} shows an example where a name is switched out with an arbitrary fake name from a provided substitution list. While this masking function leads to substantial information loss, it seemingly maintains the integrity of the data from an outsider's perspective. This can make the data easier to work with, while still ensuring anonymity. As parameters, the keys for the attributes that are supposed to be substituted are required in tandem with the intended substitutes. 

\bigskip

\begin{figure}[ht]
    \begin{center}
    \footnotesize{
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|}
            \hline
            name \\
            \hline
            Martin Smith \\
            \hline
            \end{tabular}
            \quad $\longrightarrow$ \quad
            \begin{tabular}{|c|}
            \hline
            name \\
            \hline
            John Doe \\
            \hline
        \end{tabular}
    }
    \end{center}
    \caption{Example of substitution of an attribute.\label{fig:substitution}}
\end{figure}

An alternative approach is \textbf{Tokenization}. Here, values are also substituted, but not with some arbitrary replacement. Instead, the substitute is a specific token. These tokens can be reversed to restore the original value as long as a key is known with which the token was created. There are different approaches to achieve this. The first one coming to mind is a database mapping token to the hash of the original value. Only access to the database as well as the hashing function will yield the correct original value from the token. Another approach is to omit the database and instead use a more sophisticated cryptographic algorithm to create the token, effectively encrypting the data. An additional, less computationally intensive method involves the use of a hashmap, where original values are associated with randomly generated character sequences. This approach, while simpler, still provides a level of security by obfuscating the original values. Each method has its trade-offs: while the database and cryptographic approaches ensure a robust security level, they necessitate significant storage and computing resources. On the other hand, the hashmap approach, though less resource-intensive, might offer a slightly reduced security level. This makes the choice of method dependent on the sensitivity of the data and the available resources. For instance, highly sensitive information, such as passwords, might necessitate more resource-intensive methods, as depicted in Figure \ref{fig:tokenization}.

\bigskip

\begin{figure}[ht]
    \begin{center}
    \footnotesize{
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|}
            \hline
            password \\
            \hline
            MyPassword1 \\
            \hline
            \end{tabular}
            \quad $\longrightarrow$ \quad
            \begin{tabular}{|c|}
            \hline
            password \\
            \hline
            d9f4c3b72c7935e5 \\
            \hline
        \end{tabular}
    }
    \end{center}
    \caption{Example of tokenization of an attribute.\label{fig:tokenization}}
\end{figure}

One of the most common masking functions is \textbf{Generalization}. Here, the value of an attribute is abstracted to a more general value. This effectively reduces the information - but does not remove it entirely. For example, a datum with a residency field could generalize the exact location to a broader one. Instead of Berlin, it would read Germany as shown in Figure \ref{fig:generalization}. This could then be even further generalized to Europe and so forth. Typically, entire generalization hierarchies are provided as parameters to facilitate this. These hierarchies must be exhaustive of all possible arising values if no default is provided as a generalization is ambiguous. These complete generalization hierarchies are required as parameters in addition to their respective attribute key. 

\bigskip

\begin{figure}[ht]
    \begin{center}
    \footnotesize{
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|}
            \hline
            residency \\
            \hline
            Berlin \\
            \hline
            \end{tabular}
            \quad $\longrightarrow$ \quad
            \begin{tabular}{|c|}
            \hline
            residency \\
            \hline
            Germany \\
            \hline
        \end{tabular}
    }
    \end{center}
    \caption{Example of the generalization of an attribute.\label{fig:generalization}}
\end{figure}

A special case of Generalization is \textbf{Bucketizing}. It functions similarly, but exclusively for numerical values. Ranges replace specific values in the tuple. Figure \ref{fig:bucketizing} shows an example where the age 27 is bucketized to the range {[20 - 30]}. Only a moderate amount of information is lost with this masking function. To deploy, it requires the bucket sizes as well as the attribute keys. 

\bigskip

\begin{figure}[ht]
    \begin{center}
    \footnotesize{
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|}
            \hline
            age \\
            \hline
            27 \\
            \hline
            \end{tabular}
            \quad $\longrightarrow$ \quad
            \begin{tabular}{|c|}
            \hline
            age \\
            \hline
            {[20 - 30]} \\
            \hline
        \end{tabular}
    }
    \end{center}
    \caption{Example of bucketizing of an attribute.\label{fig:bucketizing}}
\end{figure}

Finally, there are the \textbf{Noise Methods}. Again, these only apply to numerical data. The idea is to modify the original values by adding noise. Typically, this noise is chosen randomly from a distribution. The standard deviation will then define how much each data point can diverge from the original value. Utilizing the normal distribution with mean $0$ would ensure that the data over time would retain its mean and variance. The result will invalidate individual tuples but preserve the overall spread of the data in the long run. As an example, Figure \ref{fig:noise} shows noise added to two attributes of a tuple. Note that with no loss to the generality, one is decreased, while the other increases. 

\bigskip

\begin{figure}[ht]
    \begin{center}
    \footnotesize{
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|c|}
            \hline
            height & weight \\
            \hline
            185 & 83 \\
            \hline
            \end{tabular}
            \quad $\longrightarrow$ \quad
            \begin{tabular}{|c|c|}
            \hline
            height & weight  \\
            \hline
            181 & 84\\
            \hline
        \end{tabular}
    }
    \end{center}
    \caption{Example of adding noise.\label{fig:noise}}
\end{figure}

The aforementioned masking functions are applied to a single value within a record. There are more complex masking functions that go beyond this. For instance \textbf{Conditional Substitution} extends this by requiring a certain condition to be met for a masking to occur. Also, masking functions are considering multiple records at the same time. \textbf{Aggregation} is a familiar example where values of multiple records are aggregated with a specified method. One of the widely accepted models for preserving the privacy of data subjects in the dataset is \textbf{k-anonymity}. Introduced by Sweeney \cite{sweeney2002kanonymity}, the k-anonymity model necessitates that any record in a collection is indistinguishable from at least $k-1$ other records regarding their indirect identifiers. This model has been revised in the past, with further requirements as in the diversity of distinct sensitive values and maintaining the spread within the dataset within each subgroup of values. All these more complex masking functions rely upon and utilize the masking functions depicted in these subsections. They are the foundation of masking functions overall. The anonymization techniques we have just described provide further protection and are of great use at a cost of decreased performance. In Section \ref{sec:anonymization_hierarchy} we delve deeper into these more complex anonymization techniques and provide insight into their application, especially in the context of this thesis. 

\subsection{Data Streaming\label{lit:data_streaming}}
In the context of Distributed Event Stores, which operate on streaming data, additional challenges emerge, as previously established. Simple masking functions are well-suited for data stream handling, as they modify single records and integrate seamlessly into the data pipeline. Complex masking functions, such as those performing aggregations based on multiple tuples, require a distinct approach. An unbounded stream must be discretized into finite sets of records for these functions to operate effectively. Modern data stream handlers, such as \ac{DES}, typically employ 'windowing' techniques. Windowing segments an unbounded stream into discrete windows of record collections. These may vary in size, overlap, and basis (periods, sessions, or states). The scope of this concept is extensive, encompassing a vast array of methodologies and applications. Researchers have developed specialized adaptations of established concepts to address the unique challenges of streaming data. For instance, targeted solutions for k-anonymity within data streams include KIDS \cite{KIDS_zhang} and CASTLE \cite{Cao2008}. 




