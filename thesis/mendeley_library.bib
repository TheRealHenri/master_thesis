@article{RudiSynopses,
   abstract = {<p>In pursuit of real-time data analysis, approximate summarization structures, i.e., synopses, have gained importance over the years. However, existing stream processing systems, such as Flink, Spark, and Storm, do not support synopses as first class citizens, i.e., as pipeline operators. Synopses' implementation is upon users. This is mainly because of the diversity of synopses, which makes a unified implementation difficult. We present Condor, a framework that supports synopses as first class citizens. Condor facilitates the specification and processing of synopsis-based streaming jobs while hiding all internal processing details. Condor's key component is its model that represents synopses as a particular case of windowed aggregate functions. An inherent divide and conquer strategy allows Condor to efficiently distribute the computation, allowing for high-performance and linear scalability. Our evaluation shows that Condor outperforms existing approaches by up to a factor of 75x and that it scales linearly with the number of cores.</p>},
   author = {Rudi Poepsel-Lemaitre and Martin Kiefer and Joscha von Hein and Jorge-Arnulfo Quiané-Ruiz and Volker Markl},
   doi = {10.14778/3467861.3467871},
   issn = {2150-8097},
   issue = {10},
   journal = {Proceedings of the VLDB Endowment},
   month = {6},
   pages = {1818-1831},
   title = {In the land of data streams where synopses are missing, one framework to bring them all},
   volume = {14},
   year = {2021},
}
@article{RudiThesis,
   abstract = {Data exploration through visualization is a powerful technique, which is becoming
more relevant for modern big data applications. These applications create and collect
vast amounts of information, which has led to new exploration-driven systems to understand
data as it changes. However, when the data volumes grow exponentially,
which is the case of data streams, traditional data visualization tools cannot provide
fast, interactive results. Hence, middleware optimizations are essential to decrease the
response time without losing a significant amount of accuracy. State-of-the-art visualization
tools tackle this problem by providing only approximate visualizations based
on sampled data. Nevertheless, sampling algorithms represent only a small portion of
all the summaries that could be used to optimize approximate visualizations. Besides,
none of these tools are designed for streaming scenarios.
In this thesis, we propose a set of novel algorithms to generate bar charts and heatmaps
based on seven different stream summaries. Consequently, to show the performance
of our techniques, we implemented a data visualization tool, called Summ-Vis.
Summ-Vis is the first interactive data visualization tool that uses not only sampling
methods but also sketches. We decoupled the summary computation from the visualization
to offering streaming applications to manage when to submit a snapshot from
their generated synopses so that the user can freely interact with them. In this way,
Summ-Vis also becomes the first visualization tool for data streams. With an exhaustive
evaluation we show that our sketch-based methods can outperform traditional
sampling techniques for approximate data visualizations in terms of latency and accuracy
when using optimal sketch sizes. Our methods can reach an improvement of up
to four orders of magnitude in terms of absolute error for skewed data.},
   author = {Rudi Poepsel Lemaitre},
   institution = {Technische Universität Berlin},
   month = {6},
   title = {Investigating Stream Summaries for Interactive Data Visualization},
   year = {2021},
}
@article{Guo2013,
   abstract = {Research on the anonymization of static data has made great progress in recent years. Generalization and suppression are two common technologies for quasi-identifiers’ anonymization. However, the characteristics of data streams, such as potential infinity and high dynamicity, make the anonymization of data streams different from the anonymization of static data. The methods for static data anonymization cannot be directly applied to anonymizing data streams. In this paper, a novel k-anonymization approach for data streams based on clustering is proposed. In order to speed up the anonymization process and reduce the information loss, the new approach scans a stream in one turn to recognize and reuse the clusters satisfying the k-anonymity principle. The time constraints on tuple publication and cluster reuse, which are specific to data streams, are considered as well. Furthermore, the approach is improved to conform to the ℓ-diversity principle. The experiments conducted on the real datasets show that the proposed methods are both efficient and effective.},
   author = {Kun Guo and Qishan Zhang},
   doi = {10.1016/j.knosys.2013.03.007},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   month = {7},
   pages = {95-108},
   title = {Fast clustering-based anonymization approaches with time constraints for data streams},
   volume = {46},
   year = {2013},
}
@inproceedings{Beedkar2021,
   abstract = {In this paper, we address the problem of compliant geo-distributed query processing. In particular, we focus on dataflow policies that impose restrictions on movement of data across geographical or institutional borders. Traditional ways to distributed query processing do not consider such restrictions and therefore in geo-distributed environments may lead to non-compliant query execution plans. For example, an execution plan for a query over data sources from Europe, North America, and Asia, which may otherwise be optimal, may not comply with dataflow policies as a result of shipping some restricted (intermediate) data. We pose this problem of compliance in the setting of geo-distributed query processing. We propose a compliance-based query optimizer that takes into account dataflow policies, which are declaratively specified using our policy expressions, to generate compliant geo-distributed execution plans. Our experimental study using a geo-distributed adaptation of the TPC-H benchmark data indicates that our optimization techniques are effective in generating efficient compliant plans and incur low overhead on top of traditional query optimizers.},
   author = {Kaustubh Beedkar and Jorge-Arnulfo Quiané-Ruiz and Volker Markl},
   city = {New York, NY, USA},
   doi = {10.1145/3448016.3453687},
   isbn = {9781450383431},
   journal = {Proceedings of the 2021 International Conference on Management of Data},
   month = {6},
   pages = {181-193},
   publisher = {ACM},
   title = {Compliant Geo-distributed Query Processing},
   year = {2021},
}
@article{Prasser2020,
   abstract = {The race for innovation has turned into a race for data. Rapid developments of new technologies, especially in the field of artificial intelligence, are accompanied by new ways of accessing, integrating, and analyzing sensitive personal data. Examples include financial transactions, social network activities, location traces, and medical records. As a consequence, adequate and careful privacy management has become a significant challenge. New data protection regulations, for example in the EU and China, are direct responses to these developments. Data anonymization is an important building block of data protection concepts, as it allows to reduce privacy risks by altering data. The development of anonymization tools involves significant challenges, however. For instance, the effectiveness of different anonymization techniques depends on context, and thus tools need to support a large set of methods to ensure that the usefulness of data is not overly affected by risk-reducing transformations. In spite of these requirements, existing solutions typically only support a small set of methods. In this work, we describe how we have extended an open source data anonymization tool to support almost arbitrary combinations of a wide range of techniques in a scalable manner. We then review the spectrum of methods supported and discuss their compatibility within the novel framework. The results of an extensive experimental comparison show that our approach outperforms related solutions in terms of scalability and output data quality—while supporting a much broader range of techniques. Finally, we discuss practical experiences with ARX and present remaining issues and challenges ahead.},
   author = {Fabian Prasser and Johanna Eicher and Helmut Spengler and Raffael Bild and Klaus A. Kuhn},
   doi = {10.1002/spe.2812},
   issn = {0038-0644},
   issue = {7},
   journal = {Software: Practice and Experience},
   month = {7},
   pages = {1277-1304},
   title = {Flexible data anonymization using ARX—Current status and challenges ahead},
   volume = {50},
   year = {2020},
}
@article{Konstantinidis2021,
   abstract = {<p> Users have the right to consent to the use of their data, but current methods are limited to very coarse-grained expressions of consent, as "opt-in/opt-out" choices for certain uses. In this paper we identify the need for fine-grained consent management and formalize how to express and manage user consent and personal contracts of data usage in relational databases. Unlike privacy approaches, our focus is not on preserving confidentiality against an adversary, but rather cooperate with a <italic>trusted</italic> service provider to abide by user preferences in an algorithmic way. Our approach enables data owners to express the intended data usage in formal specifications, that we call <italic>consent constraints</italic> , and enables a service provider that wants to honor these constraints, to automatically do so by filtering query results that violate consent; rather than both sides relying on "terms of use" agreements written in natural language. We provide formal foundations (based on provenance), algorithms (based on unification and query rewriting), connections to data privacy, and complexity results for supporting consent in databases. We implement our framework in an open source RDBMS, and provide an evaluation against the most relevant privacy approach using the TPC-H benchmark, and on a real dataset of ICU data. </p>},
   author = {George Konstantinidis and Jet Holt and Adriane Chapman},
   doi = {10.14778/3489496.3489516},
   issn = {2150-8097},
   issue = {2},
   journal = {Proceedings of the VLDB Endowment},
   month = {10},
   pages = {375-387},
   title = {Enabling personal consent in databases},
   volume = {15},
   year = {2021},
}
@inproceedings{Cao2008,
   abstract = {Most of existing privacy preserving techniques, such as k-anonymity methods, are designed for static data sets. As such, they cannot be applied to streaming data which are continuous, transient and usually unbounded. Moreover, in streaming applications, there is a need to offer strong guarantees on the maximum allowed delay between an incoming data and its anonymized output. To cope with these requirements, in this paper, we present CASTLE (Continuously Anonymizing STreaming data via adaptive cLustEring), a cluster-based scheme that anonymizes data streams on-the-fly and, at the same time, ensures the freshness of the anonymized data by satisfying specified delay constraints. We further show how CASTLE can be easily extended to handle l-diversity [1]. Our extensive performance study shows that CASTLE is efficient and effective.},
   author = {Jianneng Cao and Barbara Carminati and Elena Ferrari and Kian Lee Tan},
   doi = {10.1109/ICDE.2008.4497561},
   isbn = {978-1-4244-1836-7},
   journal = {2008 IEEE 24th International Conference on Data Engineering},
   month = {4},
   pages = {1376-1378},
   publisher = {IEEE},
   title = {CASTLE: A delay-constrained scheme for k-anonymizing data streams},
   year = {2008},
}
@inproceedings{Otgonbayar2016,
   abstract = {Internet-of-Things (IoT) devices are capable of capturing physiological measures, location and activity information, hence sharing sensed data can lead to privacy implications. Data anonymization provides solution to this problem, however, traditional anonymization approaches only provide privacy protection for data stream generated from a single entity. Since, a single entity can make use of multiple IoT devices at an instance, IoT data streams are not fixed in nature. As conventional data stream anonymization algorithms only work on fixed width data stream they cannot be applied to IoT. In this work, we propose an anonymization algorithm for publishing IoT data streams. Our approach anonymizes tuples with similar description in a single cluster under time based sliding window. It considers similarity of tuples when clustering, and provides solution to anonymize tuples with missing values using representative values. Our experiment on real dataset shows that the proposed algorithm publishes data with less information loss and runs faster compared to conventional anonymization approaches modified to run for IoT data streams.},
   author = {Ankhbayar Otgonbayar and Zeeshan Pervez and Keshav Dahal},
   doi = {10.1109/MASS.2016.049},
   isbn = {978-1-5090-2833-7},
   journal = {2016 IEEE 13th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)},
   month = {10},
   pages = {331-336},
   publisher = {IEEE},
   title = {Toward Anonymizing IoT Data Streams via Partitioning},
   year = {2016},
}
@inbook{Sakpere2014,
   abstract = {Streaming data emerges from different electronic sources and needs to be processed in real time with minimal delay. Data streams can generate hidden and useful knowledge patterns when mined and analyzed. In spite of these benefits, the issue of privacy needs to be addressed before streaming data is released for mining and analysis purposes. In order to address data privacy concerns, several techniques have emerged. K-anonymity has received considerable attention over other privacy preserving techniques because of its simplicity and efficiency in protecting data. Yet, k-anonymity cannot be directly applied on continuous data (data streams) because of its transient nature. In this chapter, the authors discuss the challenges faced by k-anonymity algorithms in enforcing privacy on data streams and review existing privacy techniques for handling data streams. },
   author = {Aderonke B. Sakpere and Anne V. D. M. Kayem},
   doi = {10.4018/978-1-4666-6158-5.ch003},
   pages = {24-50},
   title = {A State-of-the-Art Review of Data Stream Anonymization Schemes},
   year = {2014},
}
@inproceedings{Zhang2010,
   abstract = {In a wide range of fields, data arrive in the form of high speed and huge data streams, and accompanying risks of disclosure of privacy. Most of previous studies about privacy preserving, such as k-anonymity methods, are excellent and effective, however, focus on static data sets. In this paper, we study a novel framework KIDS (K-anonymIzation Data Stream base on sliding window) to solve this problem by continuously k-anonymity on the sliding window. KIDS protects privacy of data stream well and considers the distribute density of data in data stream, thereby improve usefulness of data largely. Our theoretical analysis and experimental results show that we can receive more accurate data mining results by KIDS with high efficiency.},
   author = {Junwei Zhang and Jing Yang and Jianpei Zhang and Yongbin Yuan},
   doi = {10.1109/ICFCC.2010.5497420},
   isbn = {978-1-4244-5821-9},
   journal = {2010 2nd International Conference on Future Computer and Communication},
   pages = {V2-311-V2-316},
   publisher = {IEEE},
   title = {KIDS:K-anonymization data stream base on sliding window},
   year = {2010},
}
@book{Kleppmann2017,
   author = {Martin Kleppmann},
   title = {Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems},
   year = {2017},
}
@article{JayKrepsLOG,
   author = {Jay Kreps},
   journal = {LinkedIn Engineering},
   title = {The Log: What every software engineer should know about real-time data's unifying abstraction},
   url = {https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying},
   year = {2013},
}
@article{Ferraiolo2001,
   abstract = {<p>In this article we propose a standard for role-based access control (RBAC). Although RBAC models have received broad support as a generalized approach to access control, and are well recognized for their many advantages in performing large-scale authorization management, no single authoritative definition of RBAC exists today. This lack of a widely accepted model results in uncertainty and confusion about RBAC's utility and meaning. The standard proposed here seeks to resolve this situation by unifying ideas from a base of frequently referenced RBAC models, commercial products, and research prototypes. It is intended to serve as a foundation for product development, evaluation, and procurement specification. Although RBAC continues to evolve as users, researchers, and vendors gain experience with its application, we feel the features and components proposed in this standard represent a fundamental and stable set of mechanisms that may be enhanced by developers in further meeting the needs of their customers. As such, this document does not attempt to standardize RBAC features beyond those that have achieved acceptance in the commercial marketplace and research community, but instead focuses on defining a fundamental and stable set of RBAC components. This standard is organized into the RBAC Reference Model and the RBAC System and Administrative Functional Specification. The reference model defines the scope of features that comprise the standard and provides a consistent vocabulary in support of the specification. The RBAC System and Administrative Functional Specification defines functional requirements for administrative operations and queries for the creation, maintenance, and review of RBAC sets and relations, as well as for specifying system level functionality in support of session attribute management and an access control decision process.</p>},
   author = {David F. Ferraiolo and Ravi Sandhu and Serban Gavrila and D. Richard Kuhn and Ramaswamy Chandramouli},
   doi = {10.1145/501978.501980},
   issn = {1094-9224},
   issue = {3},
   journal = {ACM Transactions on Information and System Security},
   month = {8},
   pages = {224-274},
   title = {Proposed NIST standard for role-based access control},
   volume = {4},
   year = {2001},
}
@article{Carminati2010,
   abstract = {<p>Although access control is currently a key component of any computational system, it is only recently that mechanisms to guard against unauthorized access to streaming data have started to be investigated. To cope with this lack, in this article, we propose a general framework to protect streaming data, which is, as much as possible, independent from the target stream engine. Differently from RDBMSs, up to now a standard query language for data streams has not yet emerged and this makes the development of a general solution to access control enforcement more difficult. The framework we propose in this article is based on an expressive role-based access control model proposed by us. It exploits a query rewriting mechanism, which rewrites user queries in such a way that they do not return tuples/attributes that should not be accessed according to the specified access control policies. Furthermore, the framework contains a deployment module able to translate the rewritten query in such a way that it can be executed by different stream engines, therefore, overcoming the lack of standardization. In the article, besides presenting all the components of our framework, we prove the correctness and completeness of the query rewriting algorithm, and we present some experiments that show the feasibility of the developed techniques.</p>},
   author = {Barbara Carminati and Elena Ferrari and Jianneng Cao and Kian Lee Tan},
   doi = {10.1145/1805974.1805984},
   issn = {1094-9224},
   issue = {3},
   journal = {ACM Transactions on Information and System Security},
   month = {7},
   pages = {1-31},
   title = {A framework to enforce access control over data streams},
   volume = {13},
   year = {2010},
}
@article{HansenUnivariateMicroaggregation,
   author = {S. Lee Hansen and S. Mukherjee},
   doi = {10.1109/TKDE.2003.1209020},
   issn = {1041-4347},
   issue = {4},
   journal = {IEEE Transactions on Knowledge and Data Engineering},
   month = {7},
   pages = {1043-1044},
   title = {A polynomial algorithm for optimal univariate microaggregation},
   volume = {15},
   year = {2003},
}
